## On Using Very Large Target Vocabulary for Neural Machine Translation
##### Sebastien Jean, Kyunghyun Cho, Roland Memisevic, Yoshua Bengio

**Short Summary**: This paper introduces the idea of the *sampled softmax*, which tackles one glaring problem in neural machine translation: large target vocabularies. The target vocabularies V_T (as opposed to the input vocabulary V_I) are especially problematic because when the decoder computes the target word probabilities, it must compute a softmax over the target vocabulary of size |V_T|, which can be in the hundreds of thousands of words, which is prohibitively expensive, especially for highly inflected languages. The solution proposed is inspired from importance sampling from work by Bengio and Senecal where only some subset V_T^i is used to compute the softmax; as such, the complexity of the softmax is constant with respect to |V_T|, as opposed to scaling linearly. Results show that speed-up is achieved with good performance.

### Summary
The problem of large target vocabularies poses a particular problem for neural machine translation systems. Other non-parametric, phrase-based translation systems do not deal with this issue but are limited in other respects. NMT offers a few very desirable advantages: 1) limited domain knowledge needed, and 2) the system is jointly trained to maximize the translation performance, or log-likelihood, whereas phrase-based systems contain many feature functions that are trained separately. Previously, in Bahdanau's and Sutskever's papers, they limited the size of the target word vocabulary to a shortlist of roughly 30,000 and 80,000 common words respectively. While this was sufficient for initial experiments, an increased number of unknown words quickly degrades model performance, and is a problem that arises largely because the target word vocabulary is restricted.

There are two approaches one can take to combat the large target vocabulary problem. The first is a model-specific approach, which involves modifying the model itself. One technique is to stochastically approximate the target word probability based on noise-contrastive estimation (see [1, 2, 3] for more info). Another technique is to cluser the target words into classes so that P(y_t | y_<t, x_t) is factorized into the product of P(c_t | y_<t, x_t) and P(y_t | c_t, y_<t, x_t). This reduces the number of dot products into the sum of the number of classes and number of words per class, and while it helps speed up training, it has been shown to not be as effective during test time. The other approach to address the problem is to use a translation-specific technique. In the Luong et. al paper, they replace all unknown in the source and target sentences with <OOV_n> tokens using the word alignment model and then during decoding, replace the <OOV_n> with the respective <OOV_n> token from the source sentence.

The paper uses a model-specific approach (details in the section), where they only use a subset of V_T at each update. This allows for a use of a very large target vocabulary. Intuitively, we can think of this approach as forcing updates only on the correct word and the sampled words in its corresponding subset V_T^i. The theory is presented in the paper, but the authors go into more detail about how to use this technique practically and to have stricter guarentees on the complexity.

### Details
##### Sampled softmax
The specific problem that the naive softmax calcuation presents is that the normalizing constant of the softmax grows linearly with the size of V_T. The aim of devising a sampled softmax is to approximate the expectation of the gradient of the energy of the softmax, where the energy can be defined as the *similarity* between the target word ground truths, and the predicted probability for that target word. The authors do this by importance sampling; they simply pick some subset of samples V' from the proposal distribution Q and compute the same softmax computation over V'. However, we need to define the scheme for selecting V' in a more well-defined manner, or we lose te gains provided by this technique by not utilizing the hardware properly.

The following is done: the authors iterate through the target word vocabulary sentence by sentence and accumulate unique words in a temporary list V_i'. When this list reaches a specified size, the sentences used thus far form a partition, and the set V_i' is the smaller set from which the softmax is approximated from. This is well-defined because we can reformulate Q_i' as the new proposal distribution for each of the partitions, as it assigns equal probabiltity mass to all words in the partition, and none to words outside of the partition, as desired. Note that this choice of Q makes the estimator *biased*.

There are furthur tricks used in the paper but these are omitted as they are improvements made unique to this paper. See Sections 3.1-3.3 in the original paper for more information.

### Points of Interest
* When computing results, the authors try a multitude of different setups, and also augment the presented system with things like dictionary-lookup-based word-alignments for unknown words. It's a bit deceptive when they report the final best results because the best model is achieved with an ensemble (which is a fair thing to do, since ensembles almost always help), but it's a small quip I have with papers like this that tend to do something more intricate and powerful in the experiments than the raw mathematics/ideas presented in the paper.
* There must be a wealth of ideas that researchers have about common tricks used to train models that may not be presented in the paper but are discussed during research meetings and during the implementation. For instance, the dataset is reshuffled before each epoch, which allows words to be contrasted with different words through each epoch. In order to stabalize parameters other than word embeddings, the authors freeze the word embedding parameters and fine tune all other parameters for two additional days, which shows an improvement in BLEU score. I think practical, simple tricks like these can make a huge difference for results, especially for deep learning research.
* When they replace UNK tokens, the authors are free to use an external word-alignment model entirely, or use a translation-specific procedure to decode still unknown words, even with a larger dictionary.
* This paper is where Tensorflow's implementation of `tf.nn.sampled_softmax_loss` comes from.

### Overall Thoughts
Simply put, this paper has a neat idea for addressing the large target vocabulary problem, and they have the results to support their claims. I like the motivation from importance sampling, and I think the solution is well motivated.

### References
1. Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation.
2. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space.
3. M. Gutmann and A. Hyvarinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models.

